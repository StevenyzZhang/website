<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanzhe Zhang</title>
  
  <meta name="author" content="Yanzhe Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/gatech.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>张彦哲 (Yanzhe Zhang)</name>
              </p>
              <p> I am a fourth-year computer science Ph.D. student at <a href="https://www.ic.gatech.edu/">Georgia Tech</a>, working with <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>. I am visiting <a href="https://nlp.stanford.edu/">Stanford NLP</a> now.
              </p>
              <p>
              I received my bachelor's degree from <a href="http://www.en.cs.zju.edu.cn/">Zhejiang University</a> in 2021.
              </p>
              <p> I interned at Adobe Research (2022-2023) with <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>.
              </p>
              <p> Fun fact: मेरा नाम संजू (Sanju) है. I also go by Steven.</p>
              <p style="text-align:center">
                <a href="mailto:z_yanzhe@gatech.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=iJImxvUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/StevenyzZhang/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
            <img style="width:100%;max-width:100%" alt="profile photo" src="images/new_touxiang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
              I have been interested in the following directions:
              </p>
              <p>
                (1) To make AI capable of <strong>continually learning</strong> new tasks and knowledge.
              </p>
              <p>
                (2) To make AI more <strong>robust</strong>, <strong>fair</strong>, and <strong>safe</strong>.
              </p>
              <p>
                (3) To enable AI to benefit from and for <strong>other AI and humans</strong>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <center><img width="160" src="images/search_1.gif" alt="searchprivacyrisk_gif" style="border-style: none"></center>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2508.10880">
                <papertitle>Searching for Privacy Risks in LLM Agents via Simulation</papertitle>
              </a>
              <br>
              <strong>Yanzhe Zhang</strong>,
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
              <br>
              <em>Preprint</em>, 2025
              <br>
              <a href="https://github.com/SALT-NLP/search_privacy_risk">code</a> / <a href="data/searchprivacyrisk.bib">bibtex</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <center><img width="160" src="images/animated.gif" alt="popup" style="border-style: none"></center>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2411.02391">
                <papertitle>Attacking Vision-Language Computer Agents via Pop-ups</papertitle>
              </a>
              <br>
              <strong>Yanzhe Zhang</strong>,
              <a href="https://taoyds.github.io/">Tao Yu</a>,
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
              <br>
              <em>ACL</em>, 2025
              <br>
              <a href="https://github.com/SALT-NLP/PopupAttack">code</a> / <a href="data/popup.bib">bibtex</a>
            </td>
          </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <center><img width="100" src="images/DiVA.png" alt="diva" style="border-style: none"></center>
          </td>
          <td width="75%" valign="middle">
            <a href="https://aclanthology.org/2025.acl-long.388/">
              <papertitle>Distilling an End-to-End Voice Assistant from Speech Recognition Data</papertitle>
            </a>
            <br>
            <a href="https://williamheld.com/">Will Held</a>,
            <strong>Yanzhe Zhang</strong>,
            <a>Ella Li</a>,
            <a href="https://wyshi.github.io/">Weiyan Shi</a>,
            <a>Michael Ryan</a>,
            <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
            <br>
            <em>ACL</em>, 2025
            <br>
            <a href="https://diva-audio.github.io/">website</a> / <a href="https://github.com/Helw150/levanter/blob/will/distill/src/levanter/models/via.py">training code</a> / <a href="https://colab.research.google.com/drive/1Ab3z_BjM_FblAyne7W7hbnT6gLWOhram?usp=sharing">eval code</a> / <a href="data/diva.bib">bibtex</a>
            
          </td>
        </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <center><img width="160" src="images/sketch2code.png" alt="sketch2code" style="border-style: none"></center>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2410.16232">
                <papertitle>Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping</papertitle>
              </a>
              <br>
              <a>Ryan Li</a>,
              <strong>Yanzhe Zhang</strong>,
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
              <br>
              <em>NAACL</em>, 2025
              <br>
              <a href="https://salt-nlp.github.io/Sketch2Code-Project-Page/">website</a> / <a href="https://github.com/SALT-NLP/Sketch2Code">code</a> / <a href="data/sketch2code.bib">bibtex</a>
            </td>
          </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="130" src="images/design2code.png" alt="design2code" style="border-style: none">
        </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2403.03163">
              <papertitle>Design2Code: How Far Are We From Automating Front-End Engineering?</papertitle>
            </a>
            <br>
            <a href="https://noviscl.github.io/">Chenglei Si*</a>,
            <strong> Yanzhe Zhang* </strong>,
            <a>Ryan Li</a>,
            <a>Zhengyuan Yang</a>,
            <a>Ruibo Liu</a>,
            <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
            <br>
            <em>NAACL</em>, 2025
            <br>
            <a href="https://salt-nlp.github.io/Design2Code/">website</a> / <a href="https://github.com/NoviScl/Design2Code">code</a> / <a href="https://huggingface.co/datasets/SALT-NLP/Design2Code-hf">data</a> / <a href="data/design2code.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img height="80" src="images/dylan.png" alt="DyLAN" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2310.02170">
              <papertitle>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Evaluation</papertitle>
            </a>
            <br>
            <a href="https://www.semanticscholar.org/author/Zijun-Liu/2117942065">Zijun Liu</a>,
            <strong> Yanzhe Zhang </strong>, 
	          <a href="http://www.lpeng.net/">Peng Li</a>,
            <a href="http://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>,
            <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
            <br>
            <em>COLM</em>, 2024
            <br>
            <a href="https://github.com/SALT-NLP/DyLAN">code</a> / <a href="data/dylan.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="160" src="images/gep.png" alt="GPD" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2302.03675">
              <papertitle>Auditing Gender Presentation Differences in Text-to-Image Models</papertitle>
            </a>
            <br>
            <strong> Yanzhe Zhang </strong>, 
	          <a href="http://www.lujiang.info/index.html">Lu Jiang</a>,
            <a href="https://faculty.cc.gatech.edu/~turk/">Greg Turk</a>,
            <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
            <br>
            <em>EAAMO</em>, 2024
            <br>
            <a href="https://salt-nlp.github.io/GEP/">website</a> / <a href="https://github.com/SALT-NLP/GEP_data">code</a> / <a href="https://github.com/SALT-NLP/GEP_data">data</a> / <a href="data/GEP.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="160" src="images/llavar.png" alt="llavar" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2306.17107">
              <papertitle>Enhanced Visual Instruction Tuning for Text-rich Image Understanding</papertitle>
            </a>
            <br>
            <strong> Yanzhe Zhang </strong>, 
	          <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>,
            <a href="https://gujiuxiang.com/">Jiuxiang Gu</a>,
            <a href="https://yufanzhou.com/">Yufan Zhou</a>,
            <a href="https://research.adobe.com/person/nedim-lipka/">Nedim Lipka</a>,
            <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>,
            <a href="https://research.adobe.com/person/tong-sun/">Tong Sun</a>
            <br>
            <em>NeurIPS Workshop on Instruction Tuning and Instruction Following</em>, 2023
            <br>
            <a href="https://llavar.github.io/">website</a> / <a href="https://github.com/SALT-NLP/LLaVAR">code</a> / <a href="https://huggingface.co/datasets/SALT-NLP/LLaVAR">data</a> / <a href="data/llavar.bib">bibtex</a> / <a href="https://arxiv.org/abs/2406.06730">Improved version, CVPR 2024</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="160" src="images/RobustDemo.png" alt="RD" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2210.10693">
              <papertitle>Robustness of Demonstration-based Learning Under Limited Data Scenario</papertitle>
            </a>
            <br>
            <a href="https://icefoxzhx.github.io/">Hongxin Zhang</a>,
            <strong> Yanzhe Zhang </strong>, 
            <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>,
            <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
            <br>
            <em>EMNLP</em>, 2022
            <br>
            <a href="https://github.com/SALT-NLP/RobustDemo">code</a> / <a href="data/RobustDemo.bib">bibtex</a>
            
          </td>
        </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <img width="140" src="images/v2.gif" alt="ACM" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.10652">
                <papertitle>Continual Sequence Generation with Adaptive Compositional Modules</papertitle>
              </a>
              <br>
              <strong> Yanzhe Zhang </strong>, 
	            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&hl=en">Xuezhi Wang</a>,
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
              <br>
              <em>ACL</em>, 2022
              <br>
              <a href="https://github.com/SALT-NLP/Adaptive-Compositional-Modules">code</a> / <a href="data/CL_GEN.bib">bibtex</a>
              
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <img width="160" src="images/IDBR-2.png" alt="IDBR" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aclanthology.org/2021.naacl-main.218/">
                <papertitle>Continual Learning for Text Classification with Information Disentanglement Based Regularization</papertitle>
              </a>
              <br>
              <a href="https://codeforces.com/profile/yfhuang">Yufan Huang*</a>,
              <strong> Yanzhe Zhang* </strong>, 
              <a href="https://www.jiaaochen.com/">Jiaao Chen</a>,
	            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&hl=en">Xuezhi Wang</a>,
              <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
              <br>
              <em>NAACL</em>, 2021
              <br>
              <a href="https://github.com/SALT-NLP/IDBR">code</a> / <a href="data/NAACL2021.bib">bibtex</a>
              
            </td>
          </tr>

        </tbody></table>

				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p>
              I also participated in projects like <a href="https://swesmith.com/">SWE-Smith</a> and <a href="https://arena.xlang.ai/blog/computer-agent-arena/">Computer Agent Arena</a>.
            </p>
            <heading>Service</heading>
            <p>
              <strong>Reviewer:</strong> ARR, ACL, NAACL, EMNLP, EACL, COLM, CoLLAs, ICLR.
            </p>
          </td>
        </tr>
      </tbody></table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
