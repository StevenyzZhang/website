<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanzhe Zhang</title>
  
  <meta name="author" content="Yanzhe Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/gatech.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>张彦哲 (Yanzhe Zhang)</name>
              </p>
              <p> I am a third-year computer science Ph.D. student at <a href="https://www.ic.gatech.edu/">Georgia Tech</a>, working with <a href="http://www.diyiyang.com/">Diyi Yang</a>.
                Before coming to Georgia Tech, I received my bachelor's degree from <a href="http://www.en.cs.zju.edu.cn/">Zhejiang University</a> in 2021.
              </p>
              <p> I work as a research intern at Adobe Research (Summer 2022, Summer 2023) with <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>.
              </p>
              <p> I am visiting <a href="https://nlp.stanford.edu/">Stanford NLP</a> now. </p>
              <p> Fun fact: मेरा नाम संजू (Sanju) है. I also go by Steven.</p>
              <p style="text-align:center">
                <a href="mailto:z_yanzhe@gatech.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=iJImxvUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/StevenyzZhang/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
            <img style="width:100%;max-width:100%" alt="profile photo" src="images/new_touxiang.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                I am interested in <strong> natural language processing</strong>  and <strong> artificial intelligence</strong>, especially in the following directions:
              </p>
              <p>
                (1) To make models (agents) capable of continually learning multiple tasks and transferring knowledge.
              </p>
              <p>
                (2) To make models (agents) more robust, interpretable and efficient.
              </p>
              <p>
                (3) To enable models (agents) to benefit from and for other modalities and humans.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <center><img width="160" src="images/animated.gif" alt="popup" style="border-style: none"></center>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2411.02391">
                <papertitle>Attacking Vision-Language Computer Agents via Pop-ups</papertitle>
              </a>
              <br>
              <strong>Yanzhe Zhang</strong>,
              <a href="https://taoyds.github.io/">Tao Yu</a>,
              <a href="http://www.diyiyang.com/">Diyi Yang</a>
              <br>
              <em>Preprint</em>, 2024
              <br>
              <a href="https://github.com/SALT-NLP/PopupAttack">code</a> / <a href="data/popup.bib">bibtex</a>
            </td>
          </tr>
      
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
                <center><img width="160" src="images/sketch2code.png" alt="sketch2code" style="border-style: none"></center>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2410.16232">
                  <papertitle>Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping</papertitle>
                </a>
                <br>
                <a>Ryan Li</a>,
                <strong>Yanzhe Zhang</strong>,
                <a href="http://www.diyiyang.com/">Diyi Yang</a>
                <br>
                <em>Preprint</em>, 2024
                <br>
                <a href="https://salt-nlp.github.io/Sketch2Code-Project-Page/">website</a> / <a href="https://github.com/SALT-NLP/Sketch2Code">code</a> / <a href="data/sketch2code.bib">bibtex</a>
              </td>
            </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <center><img width="100" src="images/DiVA.png" alt="diva" style="border-style: none"></center>
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2410.02678">
              <papertitle>Distilling an End-to-End Voice Assistant from Speech Recognition Data</papertitle>
            </a>
            <br>
            <a href="https://williamheld.com/">Will Held</a>,
            <a>Ella Li</a>,
            <a>Michael Ryan</a>,
            <a href="https://wyshi.github.io/">Weiyan Shi</a>,
            <strong>Yanzhe Zhang</strong>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>Preprint</em>, 2024
            <br>
            <a href="https://diva-audio.github.io/">website</a> / <a href="https://github.com/Helw150/levanter/blob/will/distill/src/levanter/models/via.py">training code</a> / <a href="https://colab.research.google.com/drive/1Ab3z_BjM_FblAyne7W7hbnT6gLWOhram?usp=sharing">eval code</a> / <a href="data/diva.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="130" src="images/design2code.png" alt="design2code" style="border-style: none">
        </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2403.03163">
              <papertitle>Design2Code: How Far Are We From Automating Front-End Engineering?</papertitle>
            </a>
            <br>
            <a href="https://noviscl.github.io/">Chenglei Si*</a>,
            <strong> Yanzhe Zhang* </strong>,
            <a>Zhengyuan Yang</a>,
            <a>Ruibo Liu</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>Preprint</em>, 2024
            <br>
            <a href="https://salt-nlp.github.io/Design2Code/">website</a> / <a href="https://github.com/NoviScl/Design2Code">code</a> / <a href="https://huggingface.co/datasets/SALT-NLP/Design2Code-hf">data</a> / <a href="data/design2code.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img height="80" src="images/dylan.png" alt="DyLAN" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2310.02170">
              <papertitle>Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with Agent Evaluation</papertitle>
            </a>
            <br>
            <a href="https://www.semanticscholar.org/author/Zijun-Liu/2117942065">Zijun Liu</a>,
            <strong> Yanzhe Zhang </strong>, 
	          <a href="http://www.lpeng.net/">Peng Li</a>,
            <a href="http://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>COLM</em>, 2024
            <br>
            <a href="https://github.com/SALT-NLP/DyLAN">code</a> / <a href="data/dylan.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="160" src="images/gep.png" alt="GPD" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2302.03675">
              <papertitle>Auditing Gender Presentation Differences in Text-to-Image Models</papertitle>
            </a>
            <br>
            <strong> Yanzhe Zhang </strong>, 
	          <a href="http://www.lujiang.info/index.html">Lu Jiang</a>,
            <a href="https://faculty.cc.gatech.edu/~turk/">Greg Turk</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>EAAMO</em>, 2024
            <br>
            <a href="https://salt-nlp.github.io/GEP/">website</a> / <a href="https://github.com/SALT-NLP/GEP_data">code</a> / <a href="https://github.com/SALT-NLP/GEP_data">data</a> / <a href="data/GEP.bib">bibtex</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="160" src="images/llavar.png" alt="llavar" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2306.17107">
              <papertitle>Enhanced Visual Instruction Tuning for Text-rich Image Understanding</papertitle>
            </a>
            <br>
            <strong> Yanzhe Zhang </strong>, 
	          <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>,
            <a href="https://gujiuxiang.com/">Jiuxiang Gu</a>,
            <a href="https://yufanzhou.com/">Yufan Zhou</a>,
            <a href="https://research.adobe.com/person/nedim-lipka/">Nedim Lipka</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>,
            <a href="https://research.adobe.com/person/tong-sun/">Tong Sun</a>
            <br>
            <em>NeurIPS Workshop on Instruction Tuning and Instruction Following</em>, 2023
            <br>
            <a href="https://llavar.github.io/">website</a> / <a href="https://github.com/SALT-NLP/LLaVAR">code</a> / <a href="https://huggingface.co/datasets/SALT-NLP/LLaVAR">data</a> / <a href="data/llavar.bib">bibtex</a> / <a href="https://arxiv.org/abs/2406.06730">Improved version TRINS, CVPR 2024</a>
            
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
            <img width="160" src="images/RobustDemo.png" alt="RD" style="border-style: none">
          </td>
          <td width="75%" valign="middle">
            <a href="https://arxiv.org/abs/2210.10693">
              <papertitle>Robustness of Demonstration-based Learning Under Limited Data Scenario</papertitle>
            </a>
            <br>
            <a href="https://icefoxzhx.github.io/">Hongxin Zhang</a>,
            <strong> Yanzhe Zhang </strong>, 
            <a href="https://zhangry868.github.io/">Ruiyi Zhang</a>,
            <a href="http://www.diyiyang.com/">Diyi Yang</a>
            <br>
            <em>EMNLP</em>, 2022
            <br>
            <a href="https://github.com/SALT-NLP/RobustDemo">code</a> / <a href="data/RobustDemo.bib">bibtex</a>
            
          </td>
        </tr>
        
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <img width="140" src="images/v2.gif" alt="ACM" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2203.10652">
                <papertitle>Continual Sequence Generation with Adaptive Compositional Modules</papertitle>
              </a>
              <br>
              <strong> Yanzhe Zhang </strong>, 
	            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&hl=en">Xuezhi Wang</a>,
              <a href="http://www.diyiyang.com/">Diyi Yang</a>
              <br>
              <em>ACL</em>, 2022
              <br>
              <a href="https://github.com/SALT-NLP/Adaptive-Compositional-Modules">code</a> / <a href="data/CL_GEN.bib">bibtex</a>
              
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle; text-align: center;">
              <img width="160" src="images/IDBR-2.png" alt="IDBR" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aclanthology.org/2021.naacl-main.218/">
                <papertitle>Continual Learning for Text Classification with Information Disentanglement Based Regularization</papertitle>
              </a>
              <br>
              <a href="https://codeforces.com/profile/yfhuang">Yufan Huang*</a>,
              <strong> Yanzhe Zhang* </strong>, 
              <a href="https://www.jiaaochen.com/">Jiaao Chen</a>,
	            <a href="https://scholar.google.com/citations?user=ScLUQ-YAAAAJ&hl=en">Xuezhi Wang</a>,
              <a href="http://www.diyiyang.com/">Diyi Yang</a>
              <br>
              <em>NAACL</em>, 2021
              <br>
              <a href="https://github.com/SALT-NLP/IDBR">code</a> / <a href="data/NAACL2021.bib">bibtex</a>
              
            </td>
          </tr>

        </tbody></table>

				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Service</heading>
            <p>
              <strong>Volunteer:</strong> NAACL 2021.
            </p>
            <p>
              <strong>Reviewer:</strong> EMNLP 2022, ICLR 2023, EACL 2023, ACL 2023, EMNLP 2023, CoLLAs 2024, ARR (Oct 2023, Dec 2023, Feb 2024, Apr 2024, Jun 2024, Oct 2024).
            </p>
          </td>
        </tr>
      </tbody></table>
        
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
